{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pre-requisites.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "EnRfobHug_WW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# notation: vector and matrix form\n",
        "# if each element is in R, and the vector has n elements, then the vector\n",
        "# lies in the set formed by taking the Cartesian product of R n times,\n",
        "# denoted as Rn."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUZk6uJGi7XO",
        "colab_type": "text"
      },
      "source": [
        "#1. Introduction\n",
        "We will start with studying the basic concepts required to learn Deep Learning\n",
        "\n",
        "Let us try to give some definition that will be very useful for us:\n",
        "<br>\n",
        "$x_{i}$ - example\n",
        "<br>\n",
        "$y_{i}$ - target value\n",
        "<br>\n",
        "$x_{i}$ = ($x_{i1}$, ...., $x_{id}$) - features : for an image, features are intensities of every pixel on the image set.\n",
        "<br>\n",
        "$X$ - (($x_{1}$,$y_{1}$), ($x_{2}$,$y_{2}$, ..., ($x_{l}$, $y_{l}$)) - training set\n",
        "<br>\n",
        "$a(x)$ - model, hypothesis\n",
        "\n",
        "The study of linear algebra involves several types of mathematical objects:\n",
        "* scalars: \n",
        "  * A scalar is just a single number. \n",
        "  * We write in italics. \n",
        "  * We usually give scalars lowercase variables names. \n",
        "  * When we introduce them, we specify what kind of number they are. For example, we might say \"Let $s$ $\\epsilon$ $\\mathbb{R}$ be the slope of line,\" while defining a real-valued scalar.\n",
        "* vectors: \n",
        "  * A vector is an array of numbers. \n",
        "  * The number are arranged in order. \n",
        "  * We can identify each individual number by its index in that ordering.\n",
        "  * Typically we give vectors lowercase names in bold typeface, such as $x$. \n",
        "  * The element of the vector ared identified by writing its name in italic typeface, with a subscript. \n",
        "  * The first element of $x$ is $x_{1}$, the second element is $x_{2}$, and so on.\n",
        "  * We also need to say what kind of numbers are stored in the vector. If each element is in $\\mathbb{R}$, and the vector has $n$ elements, then the vector lies in the set formed by taking the Cartesian product of $\\mathbb{R}$$n$ times, denoted as $\\mathbb{R}^{n}$.\n",
        "  * We can think of vectors as identifying points in space, with each element giving the coordinate along a different axis.\n",
        "  * sometimes we need to index a set of elements of a vector. In this case, we define a set containing the indices and write the set as a subscript. For example, to access $x_{1}$, $x_{3}$, $x_{6}$, we define the set S = {1, 3, 6} and write $x_{s}$.\n",
        "  * we use the - sign to index the complement of a set. For example $x_{-1}$ is the vector containing all the elements of $x$ except for $x_{1}$, and $x_{-s}$ is the vector containing all elements of x except for $x_{1}$, $x_{3}$, $x_{6}$.\n",
        "* Matrices: \n",
        "  * A matrix is a 2-D array of numbers, so each element is identified by two indices instead of one.\n",
        "  * We usually give matrices uppercase variable names with bold typeface, such as **A**.\n",
        "  * If a real-valued matrix **A** has a height of m and a width of n, then we say that **A** $\\epsilon$ $\\mathbb{R}^{m\\times n}$\n",
        "  * we usually identify the elements of a matrix using its name in italic but not bold font, and the indices are listed with separating commas. For example, $A_{1,1}$ is the upper left entry of **A** and $A_{m,n}$ is the bottom right entry.\n",
        "  * We can identify all the numbers with vertical coordinate *i* by writing a \":\" for the horizontal coordinate. For example, $A_{i,:}$ denotes the horizontal cross section of **A** with vertical coordinate *i*. This is known as the i-th row of **A**. Likewise, $A_{:,i}$ is the i-th column of **A**.\n",
        "  * Sometimes we may need to index matrix-valued expressions that are not just a single letter. In this case, we use subscripts after the expression but do not convert anything to lowercase. For example, $f(A)_{i,j}$ gives element (i,j) of the matrix computed by applying the function $f$ to A.\n",
        "\n",
        "________________________\n",
        "One important operations on matrices is the **transpose**. The transpose of a matrix is the mirror images of the matrix across a diagonal line, called the main diagonal, running down and to the right, starting from its upper left corner. We denote the transpose of a matrix as $A^{T}$, and it is defined such that\n",
        "$(A^{T})_{i,j}$ = $A_{j,i}$\n",
        "_______________________\n",
        "**Vectors** can be thought of as **matrix** that contain only **one column**. The **transpose** of a **vector** is therefore a **matrix** with only **one row**.\n",
        "____________\n",
        "Sometimes we define a vector by writing out its elements in the text inline as a row matrix, then using the transpose operator to turn it into a standard column vector, for example $x = [x_{1}, x_{2}, x_{3}]^{T}$.\n",
        "\n",
        "## Linear model for regression\n",
        "$a(x) = b + w_{1}x_{1} + w_{2}x_{2}+...+w_{d}x_{d}$\n",
        "Where:\n",
        "* $w_{1}, ......., w_{d}$ - coefficients (weights)\n",
        "* b - bias\n",
        "* d+1 parameters\n",
        "* To make it simpler, we suppose in every sample there is a fake feature that will always have a value of one. So, a coefficient with this feature is a bias. We don't analyze bias separately, we suppose it is among the weights.\n",
        "\n",
        "In vector notation:\n",
        "<br>\n",
        "$a(x) = w_{T}x$\n",
        "\n",
        "For a sample X:\n",
        "<br>\n",
        "$a(X) = Xw$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vw3npljimMn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# how to write\n",
        "# mathematical symbols in italic\n",
        "# matrix in circular bracket"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZL_yerqj53bH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# analytical solution\n",
        "# iterative solution\n",
        "# further study of linear model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNCiMh9v75eY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# cartesian products\n",
        "# and R m times n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtjlfFEQxio2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "1de088a4-1b9d-43fa-a667-b5b53f44bcb8"
      },
      "source": [
        "import numpy as np\n",
        "X = np.random.rand(49000, 3072)\n",
        "X"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.7127627 , 0.52683909, 0.36305126, ..., 0.88511529, 0.323427  ,\n",
              "        0.45963983],\n",
              "       [0.21153714, 0.37889848, 0.74251371, ..., 0.39198457, 0.61947713,\n",
              "        0.74909212],\n",
              "       [0.04245357, 0.25263266, 0.07219145, ..., 0.36772199, 0.70291992,\n",
              "        0.97831184],\n",
              "       ...,\n",
              "       [0.14022919, 0.561953  , 0.92810499, ..., 0.03825376, 0.87987751,\n",
              "        0.41240738],\n",
              "       [0.36397976, 0.66355934, 0.11178647, ..., 0.23506835, 0.88370119,\n",
              "        0.41928292],\n",
              "       [0.99583432, 0.70290336, 0.80128041, ..., 0.38809767, 0.97495997,\n",
              "        0.57448472]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIb5Msvq41U1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "4bd39b97-57d1-4d9c-d611-b264a5fa7975"
      },
      "source": [
        "X_copy = X.copy()\n",
        "X_copy = np.hstack([X, np.ones((X_copy.shape[0], 1))])\n",
        "X_copy"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.7127627 , 0.52683909, 0.36305126, ..., 0.323427  , 0.45963983,\n",
              "        1.        ],\n",
              "       [0.21153714, 0.37889848, 0.74251371, ..., 0.61947713, 0.74909212,\n",
              "        1.        ],\n",
              "       [0.04245357, 0.25263266, 0.07219145, ..., 0.70291992, 0.97831184,\n",
              "        1.        ],\n",
              "       ...,\n",
              "       [0.14022919, 0.561953  , 0.92810499, ..., 0.87987751, 0.41240738,\n",
              "        1.        ],\n",
              "       [0.36397976, 0.66355934, 0.11178647, ..., 0.88370119, 0.41928292,\n",
              "        1.        ],\n",
              "       [0.99583432, 0.70290336, 0.80128041, ..., 0.97495997, 0.57448472,\n",
              "        1.        ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VZj_r_u40IH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# third: append the bias dimension of ones (i.e. bias trick) so that our SVM\n",
        "# only has to worry about optimizing a single weight matrix W.\n",
        "X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
        "X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
        "X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
        "X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n",
        "\n",
        "print(X_train.shape, X_val.shape, X_test.shape, X_dev.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxQwt2fppKRD",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}